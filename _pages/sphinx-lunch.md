---
layout: page
permalink: /sphinx_lunch
title: Sphinx Lunch
nav: true
order: 9
---

Welcome to the Sphinx Lunch at Carnegie Mellon University!
This lunch meeting is designed to discuss any speech-related research items regularly.
The meeting consists of the presentation by CMU faculties, CMU students, and guest speakers.
We welcome any reserach topics, including an ordinary presentation, conference presentation rehearsals, preliminary research ideas, research discussions, and so on.
We also welcome any CMU researchers and external researchers to join the meeting.

During the semester, we will regularly have the meeting in the following slot:

- Date: Thursdays 12:30pm - 1:30pm
- Room: GHC 6721

The time and room may change, especially if we have a guest speaker.
We will announce the talk information through our mailing list ([Sphinxmail: Speech Group at Carnegie Mellon](https://mailman.srv.cs.cmu.edu/mailman/listinfo/sphinxmail). Approval by admin is required).
So, please subscribe to it if you're interested in the CMU speech!

## Previous Talks
- August 8, 2022
  - Title: [An Unified Understanding of Voice Conversion and its Medical Application](https://github.com/shinjiwlab/shinjiwlab.github.io/tree/source/assets/pdf/2022-08-08-voice_conversion.pdf)
  - Speaker: Wen-Chin Huang (Nagoya University)
  - Abstract: Voice conversion (VC) is the task of converting one kind of speech to another without changing the linguistic contents, and is the second most popular research field in speech synthesis. With the rise of deep neural networks, there are more and more VC methods being proposed each year, and it might be hard to understand the difference of these methods at first sight. In this talk, I will provide my own, unified understanding of VC, and show that how most successful VC methods implement the same underlying framework. I will also introduce my recent works on dysarthric VC, as a showcase of an application of VC to medicine.
  - Bio: Wen-Chin Huang is currently a Ph.D. candidate at Nagoya University, Nagoya, Japan. He received the B.S. degree from National Taiwan University, Taipei, Taiwan, in 2018 and the M.S. degree from Nagoya University, Nagoya, Japan in 2021. He was the recipient of the Best Student Paper Award in ISCSLP2018, the Best Paper Award in APSIPA ASC 2021, and the research fellowship for young scientists (DC1) from the Japan Society for the Promotion of Science in 2021. He was a co-organizer of the Voice Conversion Challenge 2020 and VoiceMOS Challenge 2022. His research focuses on deep learning applications to speech processing, with a main focus in voice conversion and speech quality assessment.

- June 16, 2022
  - Title: Language Technology for Medical Scribing
  - Speaker: Thomas Schaaf (3M \| M\*Modal)
  - Abstract: For many reasons, physicians document what they are doing. In the past, they have used handwritten or dictated notes. With the introduction of EHR systems, the complexity of the documentation workflow has increased, leading to frustration and burnout. Medical asynchronous scribes can do the data entry and note-taking for physicians from an audio recording of the conversation between the physician and the patient. Scribes can be supported with Language Technology using a pipeline of speaker diarization, speech recognition, and natural language understanding. This enables them to asynchronously navigate the audio and review extracted dictated sections or abstractive summaries of the conversation.
  - Bio: Thomas Schaaf is a Principal Research Scientist at 3M \| M\*Modal. He received his Dr. Ing. from the Universität Karlsruhe in 2004 and has been working on Automatic Speech Recognition, Speech Translation, and Natural Language Understanding at Sony Europe, Carnegie Mellon University, Toshiba Europe, Amazon, and M\*Modal. He has worked on nearly all aspects of speech recognition systems,and his research has contributed, among others, to the prediction of word confidences, detection and learning of out-of-vocabulary words, and speaker normalization. He joined 3M in 2019 through the acquisition of M\*Modal. There, his research focuses on understanding doctor-patient conversations to reduce the burden of the documentation process for doctors and create more time to care. He is also Adjunct Faculty at the Language Technology Institute of Carnegie Mellon University and a reviewer for numerous conferences and journals.

- May 13, 2022
  - Title: Directions of Dialog Research in the Era of Big Pre-training Models
  - Speaker: Zhou Yu (Columbia University)
  - Abstract: Big pre-training models (such as BERT and GPT3) have demonstrated excellent performances on various NLP tasks. Instruction tuning and prompting have enabled these models to shine in low-resource settings. The natural question is “Will big models solve dialog tasks?” This talk will first go through big models’ impact on several sub-topics within dialog systems (e.g. social chatbots, task-oriented dialog systems, negotiation/persuasion dialog systems, continue learning in dialog systems, multilingual dialog systems, multimodal dialog systems, deployable dialog systems, etc) and then follow up with the speaker's own interpretations of the challenges remaining and possible future directions.
  - Bio: Zhou Yu joined the CS department at Columbia University in Jan 2021 as an Assistant Professor ([http://www.cs.columbia.edu/~zhouyu/](http://www.cs.columbia.edu/~zhouyu/)). Before that, she was an Assistant Professor at UC Davis. She obtained her Ph.D. from Carnegie Mellon University in 2017. Zhou has built various dialog systems that have a real impact, such as a job interview training system, a depression screening system, and a second language learning system. Her research interests include dialog systems, language understanding and generation, vision and language, human-computer interaction, and social robots. Zhou received an ACL 2019 best paper nomination, featured in Forbes 2018 30 under 30 in Science, and won the 2018 Amazon Alexa Prize.
