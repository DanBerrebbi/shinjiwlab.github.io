---
layout: post
title: PSC Usage
date: 2022-01-01 09:00:00-0800
description: PSC cluster usage.
comments: false
---

# Account creation
- Create an account for [PSC](https://portal.xsede.org/my-xsede#/guest)
- Send the username to allocation managers (e.g. Xuankai) to add the user in our group.

# login
* check https://portal.xsede.org/web/xup/single-sign-on-hub
```
$ssh -l XUPusername login.xsede.org
[XUPusername@ssohub ~]$ gsissh bridges2
[XUPusername@bridges2-login013 shared]$
```

# Misc. resources
* Bridges-2 manual: https://www.psc.edu/resources/bridges-2/user-guide-2/
* Connect using browser: https://ondemand.bridges2.psc.edu/ (optional)
* ESPnet installation guide (general purpose): https://espnet.github.io/espnet/installation.html

# Summary of GPU/RM nodes and usage
* PSC has limited service units (SUs) for GPU and RM (Regular Memory) resource availability.
* `sinfo` lists all the available partitions in PSC and their status.
* Partitions: GPU, GPU-shared, RM, RM-512, RM-shared
* In the GPU partition, each node consists of 8 v100 GPU units, and using an entire node for one hour will deduct 8 SUs from our team's GPU-AI allocation.
* There are two types of GPU nodes: `v100-16` and `v100-32` having GPU units with 16GB and 32GB memory respectively.
* To use a specific number of GPU units, request them through the GPU-shared partition by specifying `-p GPU-shared --gpus=type:n` in `sbatch` or `srun`. Here `type` can be `v100-16` or `v100-32` and `n` can range from 1 to 8.
* In the RM and RM-512 partitions, each node consists of 128 cores and using an entire node for an hour will deduct 128 SUs from our team's Regular Memory allocation.
* Nodes in RM, RM-shared partitions have a memory of 128GB, while nodes in RM-512 partitions have 512GB memory.
* To use fewer number of cores (say 4), request them through the RM-shared partition by specifying `-p RM-shared --ntasks-per-node=n` in `sbatch` or `srun`. Here `n` can range from 1 to 64.

# Important
* `Home` directory is of limited space. Please do most of your work in ocean storage (`$ cd ${PROJECT}`)
* When you publish a paper, please acknowledge the PSC. We will get benefit when we apply for PSC credits next time.
  * [Acknowledgement webpage](https://www.psc.edu/resources/bridges/acknowledgement-in-publications/)
    * Example: This work used the Extreme Science and Engineering Discovery Environment (XSEDE) ~\cite{ecss}, which is supported by National Science Foundation grant number ACI-1548562. Specifically, it used the Bridges system ~\cite{nystrom2015bridges}, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).

```
@ARTICLE{xsede,
author = {J. Towns and T. Cockerill and M. Dahan and I. Foster and K. Gaither and A. Grimshaw and V. Hazlewood and S. Lathrop and D. Lifka and G. D. Peterson and R. Roskies and J. R. Scott and N. Wilkins-Diehr},
journal = {Computing in Science \& Engineering},
title = {XSEDE: Accelerating Scientific Discovery},
year = {2014},
volume = {16},
number = {5},
pages = {62-74},
keywords={Knowledge discovery;Scientific computing;Digital systems;Materials engineering;Supercomputers},
doi = {10.1109/MCSE.2014.80},
url = {doi.ieeecomputersociety.org/10.1109/MCSE.2014.80},
ISSN = {1521-9615},
month={Sept.-Oct.}
}
@inproceedings{nystrom2015bridges,
  title={Bridges: a uniquely flexible HPC resource for new communities and data analytics},
  author={Nystrom, Nicholas A and Levine, Michael J and Roskies, Ralph Z and Scott, J Ray},
  booktitle={Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure},
  pages={1--8},
  year={2015}
}
```

# ESPnet installation steps
1. Miniconda installation
   ```bash
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
   bash Miniconda3-latest-Linux-x86_64.sh
   ```
2. Load modules and set environment
   ```bash
   module load cuda/10.2.0 cudnn mkl
   CUDAROOT=/jet/packages/spack/opt/spack/linux-centos8-zen/gcc-8.3.1/cuda-10.2.89-kz7u4ix6ed53nioz4ycqin3kujcim3bs
   ```
3. Kaldi installation:
   ```bash
   cd /ocean/projects/cis210027p/<user>
   git clone https://github.com/kaldi-asr/kaldi
   cd kaldi/tools
   make -j 8
   ./extras/install_irstlm.sh
   cd ../src/
   ./configure --use-cuda=no
   make -j clean depend; make -j 8
   ```
4. ESPNet installation:
   ```bash
   cd /ocean/projects/cis210027p/<user>
   git clone https://github.com/espnet/espnet
   cd espnet/tools/
   ln -s /ocean/projects/cis210027p/<user>/kaldi .
   . ./setup_cuda_env.sh ${CUDAROOT}
   ./setup_venv.sh $(command -v python3)
   make -j 8 CUDA_VERSION=10.2 TH_VERSION=1.8.1
   ```
5. Verify torch installation:
   ```bash
   srun --pty -p GPU-shared -N 1 --gpus=v100-16:1 /bin/bash -l
   nvidia-smi
   cd /ocean/projects/cis210027p/<user>/espnet/tools
   . ./activate_python.sh
   python -c "import torch; print(torch.cuda.is_available())"
   exit
   ```
# ESPnet usage tutorial
* Full documentation: https://espnet.github.io/espnet/tutorial.html  

## Running an example `an4` recipe:
1. Execute the data preparation stages: `-1` and `0`
   * Stage `-1` downloads and un-tars the `an4` dataset with 948 training and 130 test utterances.
   * Stage `0` prepares the dataset by creating `data/train` and `data/test` directories. Names of these directories can vary for other datasets. 
Each of these directories contains 4 files: `wav.scp`, `text`, `utt2spk` and `spk2utt`. 
A mapping from a unique utterance-ID to the utterance's filepath, text and speaker-ID is noted in `wav.scp` , `text`, `utt2spk` files respectively. An inverse mapping from the speaker-ID to the speaker's utterance-IDs is noted in `spk2utt file`.
   ```bash
   cd /ocean/projects/cis210027p/<user>/espnet
   cd egs/an4/asr1
   ./run.sh --stage -1 --stop_stage 0
   ```
2. Execute the feature extraction stage: `1`
   * Stage `1` extracts the 80 log-mel and 3 pitch features from a 25ms frame shifted every 10ms for each audio sample.
   * Parameter `nj` in this stage's code represents the number of CPUs used to parallelly extract the features.
   * Since this dataset doesn't provide a validation split, we split the 948 training utterances into 848 training and 100 dev samples.
   * A mapping from utterance-IDs to feature filepaths is noted in `data/*/feats.scp` file and extracted features are stored in the `dump/*/deltafalse/feats.{1-$nj}.ark` files.
   ```bash
   ./run.sh --stage 1 --stop_stage 1
   ```
3. Prepare a dictionary and data.json in stage: `2`
   * A mapping of each character, special tokens (ex: <unk>,<space> etc.) to a unique token-ID is stored as a dictionary at `data/lang_1char/train_nodev_units.txt`
   * Pairs of extracted features and mapped tokens (using above dictionary) for each utterance-ID is stored as a JSON file in the respective `dump` directories.
   ```bash
   ./run.sh --stage 2 --stop_stage 2
   ```
4. Train the RNN-LM and ASR models in stages: `3` and `4` 
   * Trained LM models are stored by default in `exp/train_rnnlm_pytorch_lm_word100/` directory
   * Trained ASR models are stored by default in `exp/train_nodev_pytorch_train_mtlalpha1.0/results/` directory
   * Request GPU resources for training:
   ```bash
   sbatch -t 2-00:00:00 -p GPU-shared -N 1 --gpus=v100-16:1 --mem=16G train_model.sh
   ```
   * Preview of `train_model.sh` file to train the RNN based language model (RNN-LM) in stage: `3`
   ```bash
   #!/usr/bin/env bash
   module load cuda/10.2.0 cudnn mkl
   cd /ocean/projects/cis210027p/<user>/espnet/egs/an4/asr1
   ./run.sh --stage 3 --stop_stage 3
   ```
   * Preview of `train_model.sh` file to train the ASR model in stage: `4`
   ```bash
   #!/usr/bin/env bash
   module load cuda/10.2.0 cudnn mkl
   cd /ocean/projects/cis210027p/<user>/espnet/egs/an4/asr1
   ./run.sh --stage 4 --stop_stage 4
   ```
5. Request computational resources and perform decoding in stage: `5`
   * Parameter `nj` in this stage's code represents the number of CPUs used to parallelly decode each recognition set: validation, test splits. So we must request 2x`nj` number of CPUs in total.
   * Note that for each CPU, we can request a maximum memory of 2000M.
   * Request RM-shared node resources for decoding:
   ```bash
   sbatch -t 12:00:00 -p RM-shared -N 1 --cpus-per-task=16 --mem=32000M decode_model.sh
   ```
   * Preview of `decode_model.sh` file to decode the ASR model in stage: `5`
   ```bash
   #!/usr/bin/env bash
   cd /ocean/projects/cis210027p/<user>/espnet/egs/an4/asr1
   ./run.sh --stage 5 --stop_stage 5
   ```
6. Misc.
   * An interrupted training can be resumed by specifying `--resume` parameter with the path to most recent snapshot in `exp/train_nodev_pytorch_train_mtlalpha1.0/results/` directory.
   * A unique tag for managing your experiments can be set by specifying `--lmtag` and `--tag` parameters.
   * Multi-GPU training can be done by specifying `--ngpu` in the training stages.
   * For interactive debugging purposes, `srun` command can be used to request GPU-shared or RM-shared nodes as below. Note that PSC has limited service units (SUs), so use `srun` based debugging for only the required duration.
   ```bash
   srun --pty -p GPU-shared -N 1 --gpus=v100-16:1 /bin/bash -l
   srun --pty -p RM-shared -N 1 --cpus-per-task=16 --mem=32000M /bin/bash -l
   ```
## Running an example `an4` recipe with Slurm backend
0. Modify `cmd.sh` and `conf/slurm.conf` to be ready for slurm management
   ```bash
   # cmd.sh

   # line 31
   cmd_backend='slurm'  # cmd_backend='local'
   ```
   ```
   # conf/slurm.conf

   # Default configuration                                                                                                                                                        
   command sbatch --export=PATH
   option name=* --job-name $0
   default time=48:00:00
   option time=* --time $0
   option mem=* --mem-per-cpu $0
   option mem=0
   option num_threads=* --cpus-per-task $0
   option num_threads=1 --cpus-per-task 1
   option num_nodes=* --nodes $0
   default gpu=0
   option gpu=0 -p RM-shared
   option gpu=* -p GPU-shared --gres=gpu:$0 -c $0  # Recommend allocating more CPU than, or equal to the number of GPU
   # note: the --max-jobs-run option is supported as a special case
   # by slurm.pl and you don't have to handle it in the config file.
   ```
1. Run experiments
   ```
   ./run.sh --stage 1 --stop-stage 5
   ```